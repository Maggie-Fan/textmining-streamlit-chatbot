import streamlit as st
import re
import os
import nltk
from nltk import word_tokenize
import time
from qa_utils.ckip_word_segmenter_local import LocalCkipWordSegmenter

# --- çµ±ä¸€ NLTK è³‡æ–™ç›®éŒ„ç‚º Cloud å¯ç”¨è·¯å¾‘ ---
nltk_data_path = "/home/appuser/nltk_data"
os.makedirs(nltk_data_path, exist_ok=True)
nltk.data.path.append(nltk_data_path)

# --- ç¢ºä¿ nltk å¿…è¦è³‡æº ---
nltk_packages = ['punkt', 'punkt_tab', 'stopwords', 'averaged_perceptron_tagger']
for pkg in nltk_packages:
    try:
        nltk.data.find(pkg)
    except LookupError:
        print(f"Downloading NLTK resource: {pkg}")
        nltk.download(pkg, download_dir=nltk_data_path, quiet=True)

# --- æœ¬åœ°è¼‰å…¥ CKIP word segmenter (å»¶é²åˆå§‹åŒ–) ---
def lazy_init_ckip_ws_driver():
    if "ckip_ws_driver" not in st.session_state:
        with st.spinner("ğŸ”„ Loading local CKIP word segmenter..."):

            from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker
            # st.session_state.ckip_ws_driver = CkipWordSegmenter(model="bert-base")

            st.session_state.ckip_ws_driver = LocalCkipWordSegmenter(model_path="models/ckip-models/bert-base")

            # Debug message
            # ws_driver = st.session_state.ckip_ws_driver
            # # å° tokenizer è³‡è¨Š
            # print(f"Tokenizer vocab size: {len(ws_driver.tokenizer.vocab)}")
            # print(f"Tokenizer special tokens: {ws_driver.tokenizer.special_tokens_map}")

            # # å° model è³‡è¨Š
            # print(f"Model architecture: {ws_driver.model.config.architectures}")
            # print(f"Model hidden size: {ws_driver.model.config.hidden_size}")
            # print(f"Model num_labels: {ws_driver.model.config.num_labels}")

            st.success("âœ… Local CKIP WS loaded successfully!")

# --- åœç”¨è©è¡¨ (è‡ªå®šç¾© ESG report) ---
def load_pdf_stopwords():
    pdf_stopwords = ["None", None, "n", "Col", "Table"]
    stopwords = set()
    for word in pdf_stopwords:
        if isinstance(word, str):
            stopwords.add(word.lower())
        else:
            stopwords.add(word)

    return stopwords

# --- åœç”¨è©è¡¨ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ ---
def load_chinese_stopwords(filepath='lib/chinese_stopwords.txt'):
    try:
        with open(filepath, encoding='utf-8') as f:
            stopwords = set(line.strip() for line in f if line.strip())
    except:
        stopwords = set()
    return stopwords

# --- åœç”¨è©è¡¨ (English) ---
def load_english_stopwords():
    # --- è®€å–è‹±æ–‡åœç”¨è© ---
    from nltk.corpus import stopwords
    english_stopwords = set(stopwords.words('english'))
    return english_stopwords

# --- åŸºç¤æ¸…ç† ---
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'-\s+', '', text)
    text = re.sub(r'<[^>]+>', '', text)
    return text.strip()

# --- æª¢æ¸¬èªè¨€ (ä¸­æ–‡/è‹±æ–‡) ---
def detect_pdf_language(doc, max_pages=10):
    if not doc:
        return "unknown"

    sample_text = ""
    for page_number, page in enumerate(doc):
        if page_number >= max_pages:
            break
        try:
            sample_text += page.get_text()
        except:
            continue

    chinese_chars = sum(1 for c in sample_text if '\u4e00' <= c <= '\u9fff')
    english_chars = sum(1 for c in sample_text if c.isascii() and c.isalpha())

    if chinese_chars > english_chars:
        return "chinese"
    elif english_chars > chinese_chars:
        return "english"
    else:
        return "unknown"

# --- ä¸­æ–‡å°ˆç”¨ Preprocessing ---
def preprocess_chinese_text(text):
    lazy_init_ckip_ws_driver()
    ws_driver = st.session_state.ckip_ws_driver

    start_time = time.time()

    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\u4e00-\u9fffA-Za-z]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    ws = ws_driver([text])[0]
    ws = [re.sub(r'\s+', '', w) for w in ws if w.strip()]

    # åŠ å¼·ç‰ˆä¸­æ–‡åœç”¨è©
    pdf_stopwords = load_pdf_stopwords()
    self_defined_stopwords = [
        "ä¸­", "å¹´", "å®Œæˆ", "å…±å¥½", "æœˆ", "è‘£äº‹", "è‘£äº‹æœƒ", "é›†åœ˜", "å…¬å¸", "ç›®æ¨™", "å§”å“¡æœƒ", "å…©", "é«˜",
        "ä¸»é¡Œ", "æ©Ÿåˆ¶", "æŒçºŒ", "æ", "æå", "ç™¼å±•", "è·å ´", "åƒèˆ‡", "ç¶“æ¿Ÿ", "æ ¸å¿ƒ", "ä¸­å¤®", "ç¤¾æœƒ",
        "ç®¡ç†", "ç›¸é—œ", "ç¢ºä¿", "å°ç£", "æµ·ç´", "æ¬¡", "å“¡å·¥", "å…¨çƒ", "è©•ä¼°", "ç¨½æ ¸", "å¹´åº¦", "å¹¸ç¦",
        "å…±è´", "åŒ…å®¹", "å–®ä½", "è‡³å°‘", "å®¢æˆ¶"
    ]
    chinese_stopwords = load_chinese_stopwords()
    all_stopwords = set(list(chinese_stopwords) + list(pdf_stopwords) + self_defined_stopwords)
    # for w in ws:
    #     for word in pdf_stopwords:
    #         if word == None:
    #             continue
    #         elif word.lower() in w.lower():
    #             all_stopwords.add(w)
    ws_filtered = [w for w in ws if w not in all_stopwords]

    elapsed_time = time.time() - start_time
    # print(f"Preprocess Chinese text completed in {elapsed_time:.2f} seconds.")
    return ws_filtered

# --- è‹±æ–‡å°ˆç”¨ Preprocessing ---
def preprocess_english_text(text):
    start_time = time.time()

    # --- åŸºæœ¬æ¸…ç† ---
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤ HTML tag
    text = re.sub(r'[^\u0041-\u007A]', ' ', text)  # ç§»é™¤éè‹±æ–‡å­—æ¯ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰
    text = re.sub(r'\s+', ' ', text).strip().lower()  # å»å¤šé¤˜ç©ºç™½ä¸¦è½‰å°å¯«

    # --- åˆ†è© ---
    tokens = word_tokenize(text)

    # --- åœç”¨è© ---
    pdf_stopwords = load_pdf_stopwords()
    self_defined_stopwords = [
        "company", "report", "esg", "year", "group", "goal", "committee", "ensure", "management",
        "employee", "global", "evaluate", "sustainability", "development", "responsibility",
        "stakeholder", "board", "data", "information", "page", "section"
    ]
    english_stopwords = load_english_stopwords()

    all_stopwords = set(list(english_stopwords) + list(pdf_stopwords) + self_defined_stopwords)

    # --- éæ¿¾åœç”¨è© ---
    filtered_tokens = [w for w in tokens if w.isalpha() and w not in all_stopwords]

    elapsed_time = time.time() - start_time
    # print(f"Preprocess English text completed in {elapsed_time:.2f} seconds.")

    return filtered_tokens

# --- æ“·å–æ¯é å…§å®¹ ---
def extract_text_by_page(doc, max_pages=40, skip_pages=[]):
    formatted_full_text = []
    total_items = len(doc)
    total_pages = min(total_items, max_pages)

    progress_bar = st.progress(0)
    status_text = st.empty()

    for page_number, page in enumerate(doc):
        if page_number >= max_pages:
            break
        if page_number + 1 in skip_pages:
            msg = f"â­ï¸ Skip page {page_number + 1}"
            print(msg)
            status_text.info(msg)
            continue

        try:
            text = clean_text(page.get_text())

            tables = page.find_tables()
            for table in tables:
                df = table.to_pandas()
                text += "\nTable:\n" + df.to_string() + "\n"

            print(f"Text length in page {page_number+1}: {len(text)}")

            formatted_full_text.append({
                "page": page_number + 1,
                "content": text
            })

            progress = (page_number + 1) / total_pages
            msg = f"Progress: {round(progress*100)}% | Processing {page_number+1}/{total_pages} pages"
            print(msg)
            progress_bar.progress(progress)
            status_text.info(msg)

        except Exception as e:
            error_msg = f"(extract_text_by_page) Error processing page {page_number+1}: {e}"
            print(error_msg)
            st.error(error_msg)

    print("Processing complete!")
    progress_bar.progress(1.0)
    status_text.success("âœ… PDF processing complete!")

    # è‡ªå‹•åµæ¸¬èªè¨€
    language = detect_pdf_language(doc)
    st.session_state["pdf_language"] = language
    st.info(f"ğŸŒ Detected PDF language: **{language.upper()}**")

    return formatted_full_text

# --- å–å¾— PDF å…§å®¹ ---
def get_pdf_context(page="all") -> str:
    if "pdf_text" not in st.session_state:
        return ""

    # å–å¾— PDF æŒ‡å®šé æ•¸
    if page != "all":
        for p in st.session_state["pdf_text"]:
            if p["page"] == page:
                content = p["content"]
                if p["content"] in ["", None, "None", "none"]:
                    content = "No contents have been extracted."
                return f"[Page {p['page']}]: {content}"
        return f"Page {page} not found."

    # å–å¾— PDF å…¨æ–‡
    result = []
    for p in st.session_state["pdf_text"]:
        content = p["content"]
        if content in ["", None, "None", "none"]:
            content = "No contents have been extracted."
        else:
            content = content  # é¡¯å¼å¯«å‡ºä¾†ä¾›é–±è®€
        result.append(f"[Page {p['page']}]: {content}")

    return "\n\n".join(result)

# --- PDFé è™•ç†ï¼ˆè‡ªå‹•åˆ†ä¸­æ–‡/è‹±æ–‡ï¼‰---
def preprocess_pdf_sentences(raw_text, tokenize=True):
    if not raw_text or not isinstance(raw_text, str):
        return []

    language = st.session_state.get("pdf_language", "auto")
    results = []

    page_paragraphs = raw_text.split("\n\n")

    for paragraph in page_paragraphs:
        cleaned = re.sub(r"\[Page\s*\d+\]:\s*", "", paragraph).strip()
        if not cleaned:
            continue

        if language == "chinese":
            tokens = preprocess_chinese_text(cleaned)
            if tokens:
                results.append(" ".join(tokens))
        else:
            if tokenize:
                # split_sentences = nltk.sent_tokenize(cleaned)
                # results.extend([s for s in split_sentences if s.strip()])
                tokens = preprocess_english_text(cleaned)
                results.append(" ".join(tokens))
            else:
                results.append(cleaned)

    return results
