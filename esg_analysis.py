from pdf_context import get_pdf_context, preprocess_pdf_sentences
from agents.gemini_agent import chat_with_gemini
import streamlit as st
import re
import os
import jieba
import nltk
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud
from ckip_transformers.nlp import CkipPosTagger

# è‹¥éƒ¨ç½²åœ¨ Streamlit Cloudï¼Œè‡ªå‹•åŠ è¼‰é€™å€‹è·¯å¾‘
nltk_data_path = "/home/appuser/.nltk_data"
if os.path.exists(nltk_data_path):
    nltk.data.path.append(nltk_data_path)
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

pos_tagger = CkipPosTagger()  # å»¶é²åˆå§‹åŒ–ä¸å¿…è¦

def clean_chinese_markdown_spacing(text):
    text = text.replace("ã€‚\n", "ã€‚\n\n").replace("ã€‚", "ã€‚\n")
    text = re.sub(r"(?<!\n)- ", r"\n- ", text)
    return text

def analyze_esg_from_pdf():
    pdf_text = get_pdf_context(page="all")
    language = st.session_state.get("pdf_language", "english")

    if language == "chinese":
        prompt = (
            "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ ESG å ±å‘Šåˆ†æå¸«ã€‚\n\n"
            "è«‹æ ¹æ“šä¸‹æ–¹ä¼æ¥­æ°¸çºŒå ±å‘Šçš„å…§å®¹ï¼Œåˆ†åˆ¥é‡å°ä¸‰å€‹æ§‹é¢é€²è¡Œ**æ‰¹åˆ¤æ€§åˆ†æèˆ‡é‡é»æ•´ç†**ï¼š\n"
            "1. ğŸŒ¿ ç’°å¢ƒï¼ˆEnvironmentalï¼‰ï¼šèˆ‡æ°£å€™è®Šé·ã€èƒ½æºã€ç¢³æ’ã€è³‡æºä½¿ç”¨ã€ç”Ÿç‰©å¤šæ¨£æ€§æœ‰é—œçš„æ”¿ç­–èˆ‡è¡Œå‹•\n"
            "2. ğŸ¤ ç¤¾æœƒï¼ˆSocialï¼‰ï¼šæ¶‰åŠå“¡å·¥ã€ç¤¾å€ã€å®¢æˆ¶ã€æ•™è‚²ã€å¤šå…ƒå…±èã€å“¡å·¥ç…§é¡§ç­‰äººéš›äº’å‹•é¢å‘\n"
            "3. ğŸ›ï¸ æ²»ç†ï¼ˆGovernanceï¼‰ï¼šèˆ‡å…¬å¸æ²»ç†ã€é¢¨éšªç®¡ç†ã€è‘£äº‹æœƒã€è³‡è¨Šå®‰å…¨ã€æ”¿ç­–åˆ¶å®šæœ‰é—œçš„è­°é¡Œ\n"
            "è«‹é‡å°æ¯å€‹æ§‹é¢æä¾›ä»¥ä¸‹è³‡è¨Šï¼š\n"
            "1. **æ ¸å¿ƒç­–ç•¥**ï¼šä¸€å¥è©±æè¿°è©²æ§‹é¢çš„æ•´é«”æ–¹å‘èˆ‡ç›®æ¨™\n"
            "2. **é—œéµè¡Œå‹•**ï¼šæ¢åˆ— 3~5 é …å…·é«”å¯¦è¸ä½œæ³•æˆ–æªæ–½ï¼ˆé¿å…ç©ºæ³›å£è™Ÿï¼‰\n"
            "3. **å¾…æ”¹å–„è™•**ï¼šæŒ‡å‡ºå…§å®¹ä¸­çš„ç¼ºå£ã€æ¨¡ç³Šè™•ã€ç¼ºä¹é‡åŒ–æŒ‡æ¨™ã€æˆ–éæ–¼ç± çµ±çš„éƒ¨åˆ†ï¼ˆå¦‚ç„¡å‰‡å¯« N/Aï¼‰\n\n"
            "è«‹ç”¨ä¸‹åˆ— Markdown æ ¼å¼å›æ‡‰ï¼š\n"
            "### ğŸŒ¿ ç’°å¢ƒï¼ˆEnvironmentalï¼‰\n"
            "**æ ¸å¿ƒç­–ç•¥**ï¼š...\n"
            "**é—œéµè¡Œå‹•**ï¼š\n"
            "- ...\n"
            "**å¾…æ”¹å–„è™•**ï¼š\n"
            "- ...\n\n"
            "ï¼ˆä¾åºæ¥çºŒåˆ—å‡º ç¤¾æœƒ èˆ‡ æ²»ç†ï¼‰\n\n"
            "âš ï¸ è«‹é¿å…åŒä¸€é …ç›®å‡ºç¾åœ¨å¤šå€‹æ§‹é¢ï¼Œéœ€æ ¹æ“šå…§å®¹åˆ¤æ–·æœ€åˆé©åˆ†é¡ã€‚\n"
            "âš ï¸ è‹¥ä»¥ä¸‹å ±å‘Šå…§å®¹ï¼Œä½ åˆ¤æ–·ä¸æ˜¯ä¸€å€‹ ESG å ±å‘Šï¼Œå‰‡ä¸ç”¨ç”¢å‡ºä¸Šè¿°ä¸‰å€‹æ§‹é¢çš„åˆ†æï¼Œä¸¦æé†’ä½¿ç”¨è€…ä¸Šå‚³ ESG å ±å‘Šã€‚\n"
            "ğŸ“„ å ±å‘Šå…§å®¹å¦‚ä¸‹ï¼š\n"
            f"{pdf_text}"
        )
    else:
        prompt = (
            "You are a professional ESG report analyst.\n\n"
            "Please critically analyze the following ESG report and summarize findings into the **three official ESG dimensions**:\n"
            "1. ğŸŒ¿ Environmental (E): climate change, energy, emissions, biodiversity, etc.\n"
            "2. ğŸ¤ Social (S): employee relations, diversity, education, customer/community engagement\n"
            "3. ğŸ›ï¸ Governance (G): board structure, transparency, cybersecurity, risk management, ethics\n\n"
            "For each of the three sections, return:\n"
            "- **Core Strategy**: One concise sentence that summarizes the main goal or policy direction\n"
            "- **Key Actions**: A bullet list (3â€“5 items) of clear, concrete actions or programs the company has taken.\n"
            "- **Areas for Improvement**: Any vague statements, missing indicators, repetitive info, or lack of quantitative support (write 'N/A' if none)\n\n"
            "âš ï¸ Avoid overlaps â€” each point should appear in only one category.\n"
            "âš ï¸ If applicable, comment on whether the actions include measurable KPIs, clear timelines, or observable outcomes â€” but also include meaningful qualitative efforts.\n"
            "âš ï¸ If the below content is not identified as a ESG report content, you dont have to analyze it, but gently remind users to upload ESG report.\n"
            "ğŸ“„ ESG Report Content:\n"
            f"{pdf_text}"
        )

    with st.spinner("ğŸ¤– Gemini is reading and analyzing..."):
        result = chat_with_gemini(prompt, restrict = False)

    if language == "chinese":
        result = clean_chinese_markdown_spacing(result)

    return result

def get_english_noun_adj_tokens(tokens):
    pos_tags = pos_tag(tokens)
    filtered = [word for word, pos in pos_tags if pos.startswith("NN") or pos.startswith("JJ")]
    return filtered

def show_wordcloud():
    pdf_text = get_pdf_context(page="all")
    language = st.session_state.get("pdf_language", "english")

    def plot_wordcloud(word_freq, title):
        FONT_PATH = os.path.join("fonts", "TaipeiSansTCBeta-Regular.ttf")
        print("[DEBUG] å­—å‹å­˜åœ¨ï¼Ÿ", os.path.exists("fonts/TaipeiSansTCBeta-Regular.ttf"))
        try:
            wc = WordCloud(
                font_path=FONT_PATH if language == "chinese" else None,
                width=800,
                height=500,
                background_color="white"
            ).generate_from_frequencies(word_freq)
        except Exception as e:
            wc = WordCloud(width=800, height=500, background_color="white").generate_from_frequencies(word_freq)

        st.subheader(title)
        fig, ax = plt.subplots()
        ax.imshow(wc, interpolation='bilinear')
        ax.axis("off")
        st.pyplot(fig)

    # --- å–å¾—å·²æ–·è©çš„å¥å­ ---
    sentences = preprocess_pdf_sentences(pdf_text, tokenize=True)
    if not sentences:
        st.warning("âš ï¸ No valid sentences extracted.")
        return

    # --- TF-IDF çµ±è¨ˆ ---
    tfidf = TfidfVectorizer()
    tfidf_matrix = tfidf.fit_transform(sentences)
    scores = tfidf_matrix.sum(axis=0).A1
    tokens = tfidf.get_feature_names_out()
    tfidf_dict = dict(zip(tokens, scores))

    if language == "chinese":
        words = list(tfidf_dict.keys())
        pos_tags = pos_tagger([words])[0]
        valid_pos_prefix = ("N", "V", "A")  # åè©ã€å‹•è©ã€å½¢å®¹è©
        filtered = {
            w: tfidf_dict[w]
            for w, pos in zip(words, pos_tags)
            if any(pos.startswith(p) for p in valid_pos_prefix)
        }
    else:
        filtered = tfidf_dict.copy()
        filtered = {w: tfidf_dict[w] for w in get_english_noun_adj_tokens(list(tfidf_dict.keys()))}

    plot_wordcloud(filtered, "â˜ï¸ Word Cloud (with POS)")